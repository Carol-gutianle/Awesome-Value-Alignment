# Awesome-Value-Alignment
A curated collection of resources and practical guidelines on Value Alignment research.

## Structure
![](./overview.png)

## Representations

### Value Taxonomy
- [Date] Title (Link to Pdf, Link to Code)
- []

### Value Encoding

## Interventions

### Pretrain
- [2024-12] Alignment at Pre-training! Towards Native Alignment for Arabic LLMs ([Pdf](https://arxiv.org/abs/2412.03253), [Code](https://github.com/FreedomIntelligence/AceGPT-v2))

### SFT
- [2023-08] Self-Alignment with Instruction Backtranslation ([Pdf](https://arxiv.org/abs/2308.06259))
- [2024-08] Value Alignment from Unstructured Text ([Pdf](https://www.arxiv.org/abs/2408.10392))
- [2025-03] Teaching AI to Handle Exceptions: Supervised Fine-Tuning with Human-Aligned Judgment ([Pdf](https://arxiv.org/abs/2503.02976))

### RLXF
- [2022-03] Training language models to follow instructions with human feedback ([Pdf](https://arxiv.org/abs/2203.02155))
- [2022-12] Constitutional AI: Harmlessness from AI Feedback ([Pdf](https://arxiv.org/abs/2212.08073))
- [2023-05] Language Model Self-improvement by Reinforcement Learning Contemplation ([Pdf](https://arxiv.org/abs/2305.14483))
- [2023-06] Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena([Pdf](https://arxiv.org/abs/2306.05685))
- [2023-09] RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback ([Pdf](https://arxiv.org/abs/2309.00267))
- [2024-01] Self-Rewarding Language Models ([Pdf](https://arxiv.org/abs/2401.10020))
- [2024-03] HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback ([Pdf](https://arxiv.org/abs/2403.08309))
- [2024-05] RLSF: Fine-tuning LLMs via Symbolic Feedback ([Pdf](https://arxiv.org/abs/2405.16661))
- [2024-08] Self-Taught Evaluators ([Pdf](https://arxiv.org/abs/2408.02666))
- [2024-08] Generative Verifiers: Reward Modeling as Next-Token Prediction ([Pdf](https://arxiv.org/abs/2408.15240))
- [2024-10] Generative Reward Models ([Pdf](https://arxiv.org/abs/2410.12832))
- [2025-01] Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge ([Pdf](https://arxiv.org/abs/2501.18099))
- [2025-01] DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning ([Pdf](https://arxiv.org/abs/2501.12948))
- [2025-03] Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across Diverse Domains ([Pdf](https://arxiv.org/abs/2503.23829))
- [2025-03] Beyond Verifiable Rewards: Scaling Reinforcement Learning for Language Models to Unverifiable Data ([Pdf](https://arxiv.org/abs/2503.19618v2))
- [2025-03] R-PRM: Reasoning-Driven Process Reward Modeling ([Pdf](https://arxiv.org/abs/2503.21295))
- [2025-04] Inference-Time Scaling for Generalist Reward Modeling ([Pdf](https://arxiv.org/abs/2504.02495))
- [2025-04] Process Reward Models That Think ([Pdf](https://arxiv.org/abs/2504.16828))
- [2025-04] TTRL: Test-Time Reinforcement Learning ([Pdf](https://arxiv.org/abs/2504.16084))
- [2025-05] Reinforcing General Reasoning without Verifiers ([Pdf](https://arxiv.org/abs/2505.21493))
- [2025-05] J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning ([Pdf](https://arxiv.org/abs/2505.10320))
- [2025-05] RM-R1: Reward Modeling as Reasoning ([Pdf](https://arxiv.org/abs/2505.02387))
- [2025-05] General-Reasoner: Advancing LLM Reasoning Across All Domains ([Pdf](https://arxiv.org/abs/2505.14652))
- [2025-05] X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains ([Pdf](https://arxiv.org/abs/2505.03981))
- [2025-05] Absolute Zero: Reinforced Self-play Reasoning with Zero Data([Pdf](https://arxiv.org/abs/2505.03335))
- [2025-05] Learning to Reason without External Rewards ([Pdf](https://arxiv.org/abs/2505.19590))
- [2025-06] RLPR: Extrapolating RLVR to General Domains without Verifiers ([Pdf](https://arxiv.org/abs/2506.18254))
- [2025-06] RewardAnything: Generalizable Principle-Following Reward Models ([Pdf](https://arxiv.org/abs/2506.03637))
- [2025-06] Writing-Zero: Bridge the Gap Between Non-verifiable Tasks and Verifiable Rewards ([Pdf](https://arxiv.org/abs/2506.00103))
- [2025-06] Generalist Reward Models: Found Inside Large Language Models ([Pdf](https://arxiv.org/abs/2506.23235))
- [2025-07] Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains ([Pdf](https://arxiv.org/abs/2507.17746))

### Inference-time
- [2023-12] Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations ([Pdf](https://arxiv.org/abs/2312.06674))
- [2025-05] Advancing LLM Safe Alignment with Safety Representation Ranking ([Pdf](https://arxiv.org/abs/2505.15710))

## Evaluations
- [2020-08] Aligning AI With Shared Human Values ([Pdf](https://arxiv.org/abs/2008.02275))
- [2020-08] Scruples: A Corpus of Community Ethical Judgments on 32,000 Real-Life Anecdotes ([Pdf](https://arxiv.org/abs/2008.09094))
- [2020-11] Social Chemistry 101: Learning to Reason about Social and Moral Norms ([Pdf](https://arxiv.org/abs/2011.00620))
- [2022-04] The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems ([Pdf](https://arxiv.org/abs/2204.03021))
- [2022-09] Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity ([Pdf](https://arxiv.org/abs/2209.12106))
- [2022-10] When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment ([Pdf](http://arxiv.org/abs/2210.01478))
- [2023-06] Towards Measuring the Representation of Subjective Global Opinions in Language Models ([Pdf](https://arxiv.org/abs/2306.16388))
- [2023-07] Evaluating the Moral Beliefs Encoded in LLMs ([Pdf](https://arxiv.org/abs/2307.14324))
- [2023-11] Flames: Benchmarking Value Alignment of LLMs in Chinese ([Pdf](https://arxiv.org/abs/2311.06899))
- [2023-11] Value FULCRA: Mapping Large Language Models to the Multidimensional Spectrum of Basic Human Values ([Pdf](https://aclanthology.org/2024.naacl-long.486/))
- [2024-02] KorNAT: LLM Alignment Benchmark for Korean Social Values and Common Knowledge ([Pdf](https://arxiv.org/abs/2402.13605))
- [2024-06] MoralBench: Moral Evaluation of LLMs ([Pdf](https://arxiv.org/abs/2406.04428))
- [2025-01] Mind the Value-Action Gap: Do LLMs Act in Alignment with Their Values? ([Pdf](https://arxiv.org/abs/2501.15463))
- [2025-08] Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap ([Pdf](https://arxiv.org/abs/2508.18646))
- [2025-09] EigenBench: A Comparative Behavioral Measure of Value Alignment ([Pdf](https://arxiv.org/abs/2509.01938))

## Impacts